{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Basic Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import io\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import logging\n",
    "import os\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/arjun/miniforge3/envs/ml-vector11/lib/python3.11/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration:\n",
      "   Milvus: localhost:19530\n",
      "   Collection: faq_bootcamp_collection\n",
      "   CSV File: codebasics_faqs.csv\n",
      "   Embedding Model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "MILVUS_HOST = \"localhost\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "COLLECTION_NAME = \"faq_bootcamp_collection\"\n",
    "CSV_FILE_PATH = \"codebasics_faqs.csv\"\n",
    "\n",
    "# Embedding model configuration\n",
    "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "EMBEDDING_DIM = 384\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"   Milvus: {MILVUS_HOST}:{MILVUS_PORT}\")\n",
    "print(f\"   Collection: {COLLECTION_NAME}\")\n",
    "print(f\"   CSV File: {CSV_FILE_PATH}\")\n",
    "print(f\"   Embedding Model: {EMBEDDING_MODEL_NAME}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milvus Connection Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_milvus(host=MILVUS_HOST, port=MILVUS_PORT):\n",
    "    \"\"\"Connect to Milvus database\"\"\"\n",
    "    try:\n",
    "        connections.connect(\"default\", host=host, port=port)\n",
    "        print(f\"‚úÖ Connected to Milvus at {host}:{port}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to connect to Milvus: {e}\")\n",
    "        return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Milvus at localhost:19530\n"
     ]
    }
   ],
   "source": [
    "\n",
    "connection_success = connect_to_milvus()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding Model Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(model_name=EMBEDDING_MODEL_NAME):\n",
    "    \"\"\"Load sentence transformer model for embeddings\"\"\"\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        print(f\"‚úÖ Loaded embedding model: {model_name}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load embedding model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded embedding model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "embedding_model = load_embedding_model()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milvus utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_collection_if_exists(collection_name):\n",
    "    \"\"\"Drop collection if it already exists\"\"\"\n",
    "    try:\n",
    "        if utility.has_collection(collection_name):\n",
    "            utility.drop_collection(collection_name)\n",
    "            print(f\"üóëÔ∏è Dropped existing collection: {collection_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è Collection {collection_name} doesn't exist\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking/dropping collection: {e}\")\n",
    "        return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faq_collection(collection_name, dim=EMBEDDING_DIM):\n",
    "    \"\"\"Create new collection for FAQ documents\"\"\"\n",
    "    try:\n",
    "        # Define schema\n",
    "        fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "            FieldSchema(name=\"question\", dtype=DataType.VARCHAR, max_length=2000),\n",
    "            FieldSchema(name=\"answer\", dtype=DataType.VARCHAR, max_length=4000),\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "        ]\n",
    "        \n",
    "        schema = CollectionSchema(fields, f\"FAQ Collection: {collection_name}\")\n",
    "        collection = Collection(collection_name, schema)\n",
    "        \n",
    "        # Create index for vector search\n",
    "        index_params = {\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"index_type\": \"IVF_FLAT\", \n",
    "            \"params\": {\"nlist\": 128}\n",
    "        }\n",
    "        collection.create_index(\"embedding\", index_params)\n",
    "        \n",
    "        print(f\"‚úÖ Created collection: {collection_name}\")\n",
    "        return collection\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create collection: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Dropped existing collection: faq_bootcamp_collection\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new collection\n",
    "drop_collection_if_exists(COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created collection: faq_bootcamp_collection\n"
     ]
    }
   ],
   "source": [
    "collection = create_faq_collection(COLLECTION_NAME)\n",
    "# collection = Collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faq_csv(csv_path):\n",
    "    \"\"\"Load FAQ data with encoding handling and bad line skipping\"\"\"\n",
    "    encodings = ['utf-8', 'cp1252', 'latin-1', 'iso-8859-1']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            print(f\"üîç Trying encoding: {encoding}\")\n",
    "            \n",
    "            # Try with pandas 1.3+ syntax first\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    csv_path, \n",
    "                    encoding=encoding,\n",
    "                    on_bad_lines='skip',\n",
    "                    engine='python'\n",
    "                )\n",
    "            except TypeError:\n",
    "                # Fallback for older pandas\n",
    "                df = pd.read_csv(\n",
    "                    csv_path, \n",
    "                    encoding=encoding,\n",
    "                    error_bad_lines=False,\n",
    "                    warn_bad_lines=True,\n",
    "                    engine='python'\n",
    "                )\n",
    "            \n",
    "            print(f\"‚úÖ Successfully loaded with {encoding}!\")\n",
    "            print(f\"üìä CSV Data Info:\")\n",
    "            print(f\"   Shape: {df.shape}\")\n",
    "            print(f\"   Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Display first few rows\n",
    "            print(f\"\\nüìã First 3 rows:\")\n",
    "            for i in range(min(3, len(df))):\n",
    "                print(f\"Row {i+1}:\")\n",
    "                print(f\"  Prompt: {df.iloc[i]['prompt']}\")\n",
    "                print(f\"  Response: {df.iloc[i]['response'][:100]}...\")\n",
    "                print()\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed with {encoding}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"‚ùå Failed to load CSV with any encoding\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Trying encoding: utf-8\n",
      "‚ùå Failed with utf-8: 'utf-8' codec can't decode byte 0x92 in position 1197: invalid start byte\n",
      "üîç Trying encoding: cp1252\n",
      "‚úÖ Successfully loaded with cp1252!\n",
      "üìä CSV Data Info:\n",
      "   Shape: (75, 2)\n",
      "   Columns: ['prompt', 'response']\n",
      "\n",
      "üìã First 3 rows:\n",
      "Row 1:\n",
      "  Prompt: I have never done programming in my life. Can I take this bootcamp?\n",
      "  Response: Yes, this is the perfect bootcamp for anyone who has never done coding and wants to build a career i...\n",
      "\n",
      "Row 2:\n",
      "  Prompt: Why should I trust Codebasics?\n",
      "  Response: Till now 9000 + learners have benefitted from the quality of our courses. You can check the review s...\n",
      "\n",
      "Row 3:\n",
      "  Prompt: Is there any prerequisite for taking this bootcamp ?\n",
      "  Response: Our bootcamp is specifically designed for beginners with no prior experience in this field. The only...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_faq_csv(CSV_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have never done programming in my life. Can ...</td>\n",
       "      <td>Yes, this is the perfect bootcamp for anyone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why should I trust Codebasics?</td>\n",
       "      <td>Till now 9000 + learners have benefitted from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is there any prerequisite for taking this boot...</td>\n",
       "      <td>Our bootcamp is specifically designed for begi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What datasets are used in this bootcamp? Is it...</td>\n",
       "      <td>The datasets used in this bootcamp are crafted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I‚Äôm not sure if this bootcamp is good enough f...</td>\n",
       "      <td>We got you covered. Go ahead and watch our you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>It appears that the X-axis of the chart is not...</td>\n",
       "      <td>Check this reference:\\nhttps://discordapp.com/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Why we use Net error in place of absolute net ...</td>\n",
       "      <td>Directional Insight: The net error metric offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>I am encountering an issue where the NIS IND c...</td>\n",
       "      <td>Have you taken the 'market' column from dim_ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>How do I update source in power query ?</td>\n",
       "      <td>Follow the discord link : \\n\\n https://discord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>How do I enable Power Pivot before using it fo...</td>\n",
       "      <td>Follow the process in the link : \\n\\nhttps://d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   I have never done programming in my life. Can ...   \n",
       "1                      Why should I trust Codebasics?   \n",
       "2   Is there any prerequisite for taking this boot...   \n",
       "3   What datasets are used in this bootcamp? Is it...   \n",
       "4   I‚Äôm not sure if this bootcamp is good enough f...   \n",
       "..                                                ...   \n",
       "70  It appears that the X-axis of the chart is not...   \n",
       "71  Why we use Net error in place of absolute net ...   \n",
       "72  I am encountering an issue where the NIS IND c...   \n",
       "73            How do I update source in power query ?   \n",
       "74  How do I enable Power Pivot before using it fo...   \n",
       "\n",
       "                                               answer  \n",
       "0   Yes, this is the perfect bootcamp for anyone w...  \n",
       "1   Till now 9000 + learners have benefitted from ...  \n",
       "2   Our bootcamp is specifically designed for begi...  \n",
       "3   The datasets used in this bootcamp are crafted...  \n",
       "4   We got you covered. Go ahead and watch our you...  \n",
       "..                                                ...  \n",
       "70  Check this reference:\\nhttps://discordapp.com/...  \n",
       "71  Directional Insight: The net error metric offe...  \n",
       "72  Have you taken the 'market' column from dim_ma...  \n",
       "73  Follow the discord link : \\n\\n https://discord...  \n",
       "74  Follow the process in the link : \\n\\nhttps://d...  \n",
       "\n",
       "[75 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={'prompt': 'question', 'response': 'answer'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_faq_data(df):\n",
    "    \"\"\"Clean and validate FAQ data\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "        \n",
    "    print(\"üßπ Cleaning data...\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['question', 'answer']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"‚ùå Missing required columns. Found: {list(df.columns)}\")\n",
    "        return None\n",
    "    \n",
    "    # Original shape\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    # Remove null values\n",
    "    df = df.dropna(subset=required_cols)\n",
    "    \n",
    "    # Remove empty strings\n",
    "    df = df[(df['question'].str.strip() != '') & (df['answer'].str.strip() != '')]\n",
    "    \n",
    "    # Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"   Original shape: {original_shape}\")\n",
    "    print(f\"   Clean shape: {df.shape}\")\n",
    "    print(f\"   Removed: {original_shape[0] - df.shape[0]} rows\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning data...\n",
      "   Original shape: (75, 2)\n",
      "   Clean shape: (75, 2)\n",
      "   Removed: 0 rows\n"
     ]
    }
   ],
   "source": [
    "df_clean = clean_faq_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts, model, batch_size=32):\n",
    "    \"\"\"Generate embeddings for list of texts\"\"\"\n",
    "    try:\n",
    "        print(f\"üîÑ Generating embeddings for {len(texts)} texts...\")\n",
    "        \n",
    "        embeddings = model.encode(texts, \n",
    "                                 convert_to_tensor=False,\n",
    "                                 batch_size=batch_size,\n",
    "                                 show_progress_bar=True)\n",
    "        \n",
    "        print(f\"‚úÖ Generated embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to generate embeddings: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating embeddings for 75 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea970d2d6b1409eab42aad194d1c037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated embeddings shape: (75, 384)\n"
     ]
    }
   ],
   "source": [
    "if df_clean is not None and embedding_model is not None:\n",
    "    prompts = df_clean['question'].tolist()\n",
    "    embeddings = generate_embeddings(prompts, embedding_model)\n",
    "else:\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_documents(collection, df, embeddings, batch_size=1000):\n",
    "    \"\"\"Ingest FAQ documents into Milvus collection in batches\"\"\"\n",
    "    try:\n",
    "        if collection is None or df is None or embeddings is None:\n",
    "            print(\"‚ùå Missing required components for ingestion\")\n",
    "            return False\n",
    "        \n",
    "        print(\"üì• Preparing data for batched ingestion...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        prompts = df['question'].astype(str).tolist()\n",
    "        responses = df['answer'].astype(str).tolist()\n",
    "        embeddings_list = embeddings.tolist()\n",
    "        \n",
    "        total_docs = len(prompts)\n",
    "        print(f\"   Total documents: {total_docs}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Number of batches: {(total_docs + batch_size - 1) // batch_size}\")\n",
    "        \n",
    "        # Track successful insertions\n",
    "        total_inserted = 0\n",
    "        successful_batches = 0\n",
    "        failed_batches = 0\n",
    "        all_insert_ids = []\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, total_docs, batch_size):\n",
    "            batch_num = (i // batch_size) + 1\n",
    "            end_idx = min(i + batch_size, total_docs)\n",
    "            batch_size_actual = end_idx - i\n",
    "            \n",
    "            print(f\"üîÑ Processing batch {batch_num} (documents {i+1}-{end_idx})...\")\n",
    "            \n",
    "            try:\n",
    "                # Prepare batch data\n",
    "                batch_prompts = prompts[i:end_idx]\n",
    "                batch_responses = responses[i:end_idx]\n",
    "                batch_embeddings = embeddings_list[i:end_idx]\n",
    "                \n",
    "                # Create entities for this batch\n",
    "                batch_entities = [batch_prompts, batch_responses, batch_embeddings]\n",
    "                \n",
    "                # Insert batch\n",
    "                insert_result = collection.insert(batch_entities)\n",
    "                \n",
    "                # Track success\n",
    "                inserted_count = len(insert_result.primary_keys)\n",
    "                total_inserted += inserted_count\n",
    "                successful_batches += 1\n",
    "                all_insert_ids.extend(insert_result.primary_keys)\n",
    "                \n",
    "                print(f\"   ‚úÖ Batch {batch_num}: {inserted_count} documents inserted\")\n",
    "                \n",
    "            except Exception as batch_error:\n",
    "                print(f\"   ‚ùå Batch {batch_num} failed: {batch_error}\")\n",
    "                failed_batches += 1\n",
    "                continue\n",
    "        \n",
    "        # Load collection to memory after all batches\n",
    "        print(\"üîÑ Loading collection to memory...\")\n",
    "        collection.load()\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nüìä Ingestion Summary:\")\n",
    "        print(f\"   Total documents processed: {total_docs}\")\n",
    "        print(f\"   Successfully inserted: {total_inserted}\")\n",
    "        print(f\"   Failed documents: {total_docs - total_inserted}\")\n",
    "        print(f\"   Successful batches: {successful_batches}\")\n",
    "        print(f\"   Failed batches: {failed_batches}\")\n",
    "        print(f\"   Success rate: {(total_inserted/total_docs)*100:.1f}%\")\n",
    "        \n",
    "        if total_inserted > 0:\n",
    "            print(f\"‚úÖ Ingestion completed with {total_inserted} documents\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå No documents were successfully inserted\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to ingest documents: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Preparing data for batched ingestion...\n",
      "   Total documents: 75\n",
      "   Batch size: 1000\n",
      "   Number of batches: 1\n",
      "üîÑ Processing batch 1 (documents 1-75)...\n",
      "   ‚úÖ Batch 1: 75 documents inserted\n",
      "üîÑ Loading collection to memory...\n",
      "\n",
      "üìä Ingestion Summary:\n",
      "   Total documents processed: 75\n",
      "   Successfully inserted: 75\n",
      "   Failed documents: 0\n",
      "   Successful batches: 1\n",
      "   Failed batches: 0\n",
      "   Success rate: 100.0%\n",
      "‚úÖ Ingestion completed with 75 documents\n"
     ]
    }
   ],
   "source": [
    "if collection and df_clean is not None and embeddings is not None:\n",
    "    ingestion_success = ingest_documents(collection, df_clean, embeddings)\n",
    "else:\n",
    "    ingestion_success = False\n",
    "    print(\"‚ùå Cannot proceed with ingestion - missing components\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_stats(collection, collection_name):\n",
    "    \"\"\"Get and display collection statistics\"\"\"\n",
    "    try:\n",
    "        if collection is None:\n",
    "            print(\"‚ùå No collection available\")\n",
    "            return 0\n",
    "            \n",
    "        # Get collection info\n",
    "        num_entities = collection.num_entities\n",
    "        \n",
    "        print(\"üìä Collection Statistics:\")\n",
    "        print(f\"   Collection Name: {collection_name}\")\n",
    "        print(f\"   Total Documents: {num_entities}\")\n",
    "        print(f\"   Status: {'Loaded' if collection.is_loaded else 'Not Loaded'}\")\n",
    "        \n",
    "        return num_entities\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to get collection stats: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Refactored FAQ System using Phi-3 Mini for CPU inference\n",
    "Replace your existing LLM code with this optimized version\n",
    "\"\"\"\n",
    "\n",
    "def load_phi3_model():\n",
    "    \"\"\"Load Microsoft Phi-3 Mini - excellent for CPU inference\"\"\"\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        import torch\n",
    "        \n",
    "        print(\"üß† Loading Microsoft Phi-3 Mini (3.8B params - CPU optimized)...\")\n",
    "        \n",
    "        model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Load model optimized for CPU\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32,  # float32 better for CPU\n",
    "            device_map=\"cpu\",\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        # Set pad token\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        print(\"‚úÖ Phi-3 Mini loaded successfully on CPU!\")\n",
    "        return tokenizer, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading Phi-3: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def generate_answer_phi3(query, relevant_faqs, tokenizer, model):\n",
    "    \"\"\"Generate answer using Phi-3 Mini - replaces your generate_answer function\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        \n",
    "        if not relevant_faqs:\n",
    "            return \"I don't have information to answer that question.\"\n",
    "        \n",
    "        # Create focused prompt for Phi-3\n",
    "        context = f\"Relevant FAQ: {relevant_faqs[0]['response']}\"\n",
    "        if len(relevant_faqs) > 1:\n",
    "            context += f\"\\nAdditional info: {relevant_faqs[1]['response']}\"\n",
    "        \n",
    "        # Phi-3 chat format\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful customer service assistant for a data analytics bootcamp. Use the provided FAQ information to answer user questions concisely and accurately.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"FAQ Information:\\n{context}\\n\\nUser Question: {query}\\n\\nProvide a helpful answer based on the FAQ information:\"}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=1024,  # Keep context manageable for CPU\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Generate with CPU-optimized settings\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.3,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        \n",
    "        # Extract response\n",
    "        response = tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        print('response', response)\n",
    "        \n",
    "        return response if response else f\"Based on our FAQ: {relevant_faqs[0]['response']}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Phi-3 generation failed: {e}\")\n",
    "        return f\"Based on our FAQ: {relevant_faqs[0]['response']}\" if relevant_faqs else \"I couldn't generate an answer.\"\n",
    "\n",
    "def generate_simple_answer(query, relevant_faqs):\n",
    "    \"\"\"Generate simple rule-based answer (fallback method)\"\"\"\n",
    "    \n",
    "    if not relevant_faqs:\n",
    "        return \"I don't have information to answer that question.\"\n",
    "    \n",
    "    # Get the best match\n",
    "    best_match = relevant_faqs[0]\n",
    "    \n",
    "    # Create contextual answer\n",
    "    if best_match['score'] > 0.8:\n",
    "        # High similarity - direct answer\n",
    "        answer = f\"Based on our FAQ: {best_match['response']}\"\n",
    "    elif best_match['score'] > 0.6:\n",
    "        # Medium similarity - contextual answer\n",
    "        answer = f\"Here's related information from our FAQ: {best_match['response']}\"\n",
    "        \n",
    "        # Add second best if available and relevant\n",
    "        if len(relevant_faqs) > 1 and relevant_faqs[1]['score'] > 0.5:\n",
    "            answer += f\"\\n\\nAdditionally: {relevant_faqs[1]['response']}\"\n",
    "    else:\n",
    "        # Low similarity - general response\n",
    "        answer = f\"I found some related information: {best_match['response']}\"\n",
    "        answer += f\"\\n\\nIf this doesn't fully answer your question, please contact our support team for more specific information.\"\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "def search_faqs(collection, query, embedding_model, top_k=3):\n",
    "    \"\"\"Search for similar FAQs\"\"\"\n",
    "    try:\n",
    "        if collection is None or embedding_model is None:\n",
    "            print(\"‚ùå Missing collection or embedding model\")\n",
    "            return []\n",
    "        \n",
    "        # Generate embedding for query\n",
    "        query_embedding = embedding_model.encode([query])[0]\n",
    "        \n",
    "        # Search parameters\n",
    "        search_params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
    "        \n",
    "        # Perform search\n",
    "        results = collection.search(\n",
    "            data=[query_embedding],\n",
    "            anns_field=\"embedding\",\n",
    "            param=search_params, \n",
    "            limit=top_k,\n",
    "            output_fields=[\"prompt\", \"response\"]\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        search_results = []\n",
    "        for i, result in enumerate(results[0]):\n",
    "            search_results.append({\n",
    "                'rank': i + 1,\n",
    "                'prompt': result.entity.get('prompt'),\n",
    "                'response': result.entity.get('response'), \n",
    "                'score': float(result.score),\n",
    "                'id': result.id\n",
    "            })\n",
    "        \n",
    "        return search_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def display_search_results(query, results):\n",
    "    \"\"\"Display search results in a formatted way\"\"\"\n",
    "    print(f\"üîç Search Query: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå No results found\")\n",
    "        return\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"üìç Rank {result['rank']} (Similarity: {result['score']:.3f})\")\n",
    "        print(f\"Q: {result['prompt']}\")\n",
    "        print(f\"A: {result['response'][:150]}{'...' if len(result['response']) > 150 else ''}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def answer_question_with_phi3(collection, query, embedding_model, tokenizer=None, model=None, top_k=3):\n",
    "    \"\"\"Complete FAQ answering pipeline with Phi-3 generation - replaces your answer_question_with_llm\"\"\"\n",
    "    \n",
    "    print(f\"üß† Phi-3 processing query: '{query}'\")\n",
    "    \n",
    "    # Step 1: Retrieve relevant FAQs\n",
    "    relevant_faqs = search_faqs(collection, query, embedding_model, top_k=top_k)\n",
    "    \n",
    "    if not relevant_faqs:\n",
    "        return {\n",
    "            'query': query,\n",
    "            'answer': \"I couldn't find relevant information for your question.\",\n",
    "            'relevant_faqs': [],\n",
    "            'method': 'no_results'\n",
    "        }\n",
    "    \n",
    "    # Step 2: Check for high similarity - use direct FAQ response\n",
    "    if relevant_faqs[0]['score'] > 0.85:\n",
    "        return {\n",
    "            'query': query,\n",
    "            'answer': f\"Based on our FAQ: {relevant_faqs[0]['response']}\",\n",
    "            'relevant_faqs': relevant_faqs,\n",
    "            'method': 'direct_match',\n",
    "            'status': 'success'\n",
    "        }\n",
    "    \n",
    "    # Step 3: Generate answer using Phi-3 or fallback\n",
    "    if tokenizer and model:\n",
    "        answer = generate_answer_phi3(query, relevant_faqs, tokenizer, model)\n",
    "        method = 'phi3_cpu'\n",
    "    else:\n",
    "        answer = generate_simple_answer(query, relevant_faqs)\n",
    "        method = 'rule_based'\n",
    "    \n",
    "    return {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'relevant_faqs': relevant_faqs,\n",
    "        'method': method,\n",
    "        'status': 'success'\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Setting up Phi-3 Mini for CPU inference...\n",
      "üß† Loading Microsoft Phi-3 Mini (3.8B params - CPU optimized)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f0f9720ba34175ac8dd02d24b2cf8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Phi-3 Mini loaded successfully on CPU!\n"
     ]
    }
   ],
   "source": [
    "# Load Phi-3 model (replaces your llm_tokenizer, llm_model loading)\n",
    "print(\"üöÄ Setting up Phi-3 Mini for CPU inference...\")\n",
    "phi3_tokenizer, phi3_model = load_phi3_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries (same as yours)\n",
    "test_queries_llm = [\n",
    "    \"Can I take this course without any programming background?\",\n",
    "    \"What happens if I'm not satisfied with the bootcamp?\",\n",
    "    \"How much time do I need to dedicate daily?\",\n",
    "    \"Will you help me get a job after completion?\",\n",
    "    \"csds\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Testing Complete FAQ System with Phi-3 Mini:\n",
      "======================================================================\n",
      "üß† Phi-3 processing query: 'Can I take this course without any programming background?'\n",
      "\n",
      "1. ‚ùì Query: Can I take this course without any programming background?\n",
      "   ‚è±Ô∏è  Time: 2.46 seconds\n",
      "   üéØ Method: direct_match\n",
      "   üí¨ Answer: Based on our FAQ: Yes, this is the perfect course for anyone who has never done coding and wants to build a career in the IT/Data Analytics industry or just wants to perform better in their current job or business using data.\n",
      "   üìä Retrieved 3 relevant FAQs\n",
      "--------------------------------------------------\n",
      "üß† Phi-3 processing query: 'What happens if I'm not satisfied with the bootcamp?'\n",
      "\n",
      "2. ‚ùì Query: What happens if I'm not satisfied with the bootcamp?\n",
      "   ‚è±Ô∏è  Time: 0.24 seconds\n",
      "   üéØ Method: direct_match\n",
      "   üí¨ Answer: Based on our FAQ: As promised we will give you a 100% refund based on the guidelines (Please refer to our course refund policy before enrolling).\n",
      "   üìä Retrieved 3 relevant FAQs\n",
      "--------------------------------------------------\n",
      "üß† Phi-3 processing query: 'How much time do I need to dedicate daily?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "# Refactored test loop using Phi-3\n",
    "if collection and embedding_model:\n",
    "    print(\"\\nüß† Testing Complete FAQ System with Phi-3 Mini:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    import time\n",
    "    total_time = 0\n",
    "    \n",
    "    for i, query in enumerate(test_queries_llm, 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use Phi-3 function instead of your original function\n",
    "        result = answer_question_with_phi3(\n",
    "            collection, \n",
    "            query, \n",
    "            embedding_model, \n",
    "            phi3_tokenizer, \n",
    "            phi3_model, \n",
    "            top_k=3\n",
    "        )\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        total_time += duration\n",
    "        \n",
    "        print(f\"\\n{i}. ‚ùì Query: {query}\")\n",
    "        print(f\"   ‚è±Ô∏è  Time: {duration:.2f} seconds\")\n",
    "        print(f\"   üéØ Method: {result['method']}\")\n",
    "        print(f\"   üí¨ Answer: {result['answer']}\")\n",
    "        print(f\"   üìä Retrieved {len(result['relevant_faqs'])} relevant FAQs\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Performance summary\n",
    "    avg_time = total_time / len(test_queries_llm)\n",
    "    print(f\"\\nüìà Performance Summary:\")\n",
    "    print(f\"   Model: Phi-3 Mini (3.8B params)\")\n",
    "    print(f\"   Average response time: {avg_time:.2f} seconds\")\n",
    "    print(f\"   Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"   Device: CPU\")\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Collection or embedding_model not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-vector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
